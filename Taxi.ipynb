{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Reinforcement Learning with Taxi V2 OpenAI Gym**\n",
    "\n",
    "We shall use the Taxi V2 Open AI gym library. \n",
    "The Documentation can be found at: https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import datetime\n",
    "import keras \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "from time import sleep\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Intro to taxi game environment**\n",
    "\n",
    "The aim of the taxi game is to make sure the taxi can get to the passenger, pick him up and bring him to the drop-off location in the fastest way possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representations**\n",
    "\n",
    "- | --> WALL (Can't pass through, will remain in the same position if tries to move through wall)\n",
    "\n",
    "- Yellow --> Taxi Current Location\n",
    "\n",
    "- Blue --> Pick up Location\n",
    "\n",
    "- Purple --> Drop-off Location\n",
    "\n",
    "- Green --> Taxi turn green once passenger board\n",
    "\n",
    "- Letters --> Locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Number of states in the environment = **500** (0 to 499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n   #Total no. of states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actions (6 in total)**\n",
    "\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n   #Total no. of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.env.s = 122\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "\n",
    "The 4 elements returned are:\n",
    "\n",
    "-  **Observation (object)**: the state the environment is in or an environment-specific object representing your observation of the environment.\n",
    "-  **Reward (float)**: Reward achieved by the previous action. \n",
    "    -  +20: Last step when we successfully pick up a passenger and drop them off at their desired location\n",
    "    -  -1: for each step in order for the agent to try and find the quickest solution possible\n",
    "    -  -10: every time you incorrectly pick up or drop off a passenger\n",
    "-  **Done (boolean)**: whether itâ€™s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, you lost your last life.)\n",
    "-  **Info (dict)**: Can be ignored, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "env.render()  #view state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function (env.P) below can be used to see the relevant states and rewards for each action taken in that particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 400, -1, False)],\n",
       " 1: [(1.0, 200, -1, False)],\n",
       " 2: [(1.0, 300, -1, False)],\n",
       " 3: [(1.0, 300, -1, False)],\n",
       " 4: [(1.0, 300, -10, False)],\n",
       " 5: [(1.0, 300, -10, False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.P[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Random Search**\n",
    "Let's start with the simplest way to train our agent to complete this task. The agent would just take random steps at every state until he completes the task (picking the passenger and dropping him off at the drop-off location). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "random_policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\n",
    "def random_policy_steps_count():\n",
    "    state = env.reset()\n",
    "    counter = 0\n",
    "    reward = None\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(env.action_space.sample())  \n",
    "        counter += 1\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angps/anaconda3/envs/angps/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Distribution of number of steps needed')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = [random_policy_steps_count() for i in range(1000)]\n",
    "sns.distplot(counts)\n",
    "plt.title(\"Distribution of number of steps needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An agent using Random search takes about an average of 2379 steps to successfully complete its mission.\n"
     ]
    }
   ],
   "source": [
    "print(\"An agent using Random search takes about an average of \" + str(int(np.mean(counts)))\n",
    "      + \" steps to successfully complete its mission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, it is not the most efficient way to complete this. Let's try to use policy iteration and value iteration method to more effectively let our agent complete this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Policy Iteration/Value Iteration**\n",
    "\n",
    "**Policy Iteration** --> The algorithm redefines the policy at each step and improve the policy, and then compute the value according to this new policy until the policy converges.\n",
    "\n",
    "**Value Iteration** --> The algorithm computes the optimal state value function by iteratively improving the estimate of V(s). The algorithm initialize V(s) to arbitrary random values. It repeatedly updates the Q(s, a) and V(s) values until they converge. \n",
    "\n",
    "\n",
    "Policy iteration and Value Iteration are guranteed to convege to the optimal policy and it often takes less iterations for policy iteration to converge than the value-iteration algorithm.\n",
    "\n",
    "Both value-iteration and policy-iteration algorithms can be used for offline planning where the agent is assumed to have prior knowledge about the effects of its actions on the environment (they assume the Markov Decision model is known). Comparing to each other, policy-iteration is computationally efficient as it often takes considerably fewer number of iterations to converge although each iteration is more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Functions for policy evaluation, policy iteration and value iteration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.env.nS)\n",
    "    while True:\n",
    "        # TODO: Implement!\n",
    "        delta = 0  #delta = change in value of state from one iteration to next\n",
    "       \n",
    "        for state in range(env.env.nS):  #for all states\n",
    "            val = 0  #initiate value as 0\n",
    "            \n",
    "            for action,act_prob in enumerate(policy[state]): #for all actions/action probabilities\n",
    "                for prob,next_state,reward,done in env.env.P[state][action]:  #transition probabilities,state,rewards of each action\n",
    "                    val += act_prob * prob * (reward + discount_factor * V[next_state])  #eqn to calculate\n",
    "            delta = max(delta, np.abs(val-V[state]))\n",
    "            V[state] = val\n",
    "        if delta < theta:  #break if the change in value is less than the threshold (theta)\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "def policy_iteration(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.env.nA)\n",
    "        for a in range(env.env.nA):\n",
    "            for prob, next_state, reward, done in env.env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\n",
    "\n",
    "    while True:\n",
    "        # Implement this!\n",
    "        curr_pol_val = policy_eval_fn(policy, env, discount_factor)  #eval current policy\n",
    "        policy_stable = True  #Check if policy did improve (Set it as True first)\n",
    "        for state in range(env.env.nS):  #for each states\n",
    "            chosen_act = np.argmax(policy[state])  #best action (Highest prob) under current policy\n",
    "            act_values = one_step_lookahead(state,curr_pol_val)  #use one step lookahead to find action values\n",
    "            best_act = np.argmax(act_values) #find best action\n",
    "            if chosen_act != best_act:\n",
    "                policy_stable = False  #Greedily find best action\n",
    "            policy[state] = np.eye(env.env.nA)[best_act]  #update \n",
    "        if policy_stable:\n",
    "            return policy, curr_pol_val\n",
    "    \n",
    "    return policy, np.zeros(env.env.nS)\n",
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.env.nA)\n",
    "        for act in range(env.env.nA):\n",
    "            for prob, next_state, reward, done in env.env.P[state][act]:\n",
    "                A[act] += prob * (reward + discount_factor*V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.env.nS)\n",
    "    while True:\n",
    "        delta = 0  #checker for improvements across states\n",
    "        for state in range(env.env.nS):\n",
    "            act_values = one_step_lookahead(state,V)  #lookahead one step\n",
    "            best_act_value = np.max(act_values) #get best action value\n",
    "            delta = max(delta,np.abs(best_act_value - V[state]))  #find max delta across all states\n",
    "            V[state] = best_act_value  #update value to best action value\n",
    "        if delta < theta:  #if max improvement less than threshold\n",
    "            break\n",
    "    policy = np.zeros([env.env.nS, env.env.nA])\n",
    "    for state in range(env.env.nS):  #for all states, create deterministic policy\n",
    "        act_val = one_step_lookahead(state,V)\n",
    "        best_action = np.argmax(act_val)\n",
    "        policy[state][best_action] = 1\n",
    "        \n",
    "    \n",
    "    # Implement!\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the random policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-64.52483596, -71.98303998, -71.59548842, -72.0983153 ,\n",
       "       -79.64362433, -79.32480882, -79.64974547, -79.63131331,\n",
       "       -79.05557089, -79.10000212, -77.9706677 , -79.10335244,\n",
       "       -79.81584076, -79.80701521, -79.81712685, -79.63332039,\n",
       "       -54.13993621, -71.16899479, -70.28411232, -71.43219898,\n",
       "       -71.25030023, -75.46716562, -75.24804402, -75.53234223,\n",
       "       -79.57512987, -79.19501136, -79.582428  , -79.56045159,\n",
       "       -79.21609557, -79.25297406, -78.31561183, -79.25575487,\n",
       "       -79.7862176 , -79.77597107, -79.78771075, -79.57431099,\n",
       "       -64.88302165, -74.00606431, -73.45166685, -74.40668129,\n",
       "       -78.74777188, -79.3512305 , -79.3198729 , -79.36055765,\n",
       "       -77.8651867 , -75.95477715, -77.90186579, -77.79141546,\n",
       "       -79.71322567, -79.72671362, -79.38388062, -79.72773067,\n",
       "       -79.47239266, -79.44709493, -79.47607904, -78.94922067,\n",
       "       -75.48283869, -72.17301644, -76.28261086, -75.78218888,\n",
       "       -79.10235711, -79.53492541, -79.51244783, -79.54161125,\n",
       "       -76.42105744, -73.21824878, -76.48255009, -76.29737963,\n",
       "       -79.78564177, -79.79572262, -79.53948965, -79.79648276,\n",
       "       -79.34485432, -79.31343978, -79.34943202, -78.69518405,\n",
       "       -74.76523715, -67.83099514, -75.2819291 , -74.4224614 ,\n",
       "       -79.30934674, -79.64215733, -79.62486348, -79.6473013 ,\n",
       "       -72.53198866, -65.84873413, -72.66030453, -72.27391188,\n",
       "       -79.8302752 , -79.83825607, -79.63539953, -79.83885786,\n",
       "       -79.27045364, -79.23547083, -79.27555131, -78.54698988,\n",
       "       -71.76084759, -55.90919475, -72.06519324, -71.14872912,\n",
       "       -72.77111042, -76.25502043, -76.07398549, -76.30886834,\n",
       "       -79.5996316 , -79.24144027, -79.60650874, -79.58580005,\n",
       "       -78.59685619, -78.66287112, -76.98493319, -78.66784895,\n",
       "       -79.7873613 , -79.77716962, -79.78884646, -79.57658882,\n",
       "       -66.80950855, -74.20286377, -72.21072946, -74.5597012 ,\n",
       "       -75.21274115, -77.51990944, -77.40002169, -77.55556943,\n",
       "       -79.37250769, -78.81104856, -79.38328748, -79.35082683,\n",
       "       -79.12911216, -79.17008375, -78.128685  , -79.1731732 ,\n",
       "       -79.68912635, -79.67422303, -79.69129807, -79.38091538,\n",
       "       -70.85236147, -74.95034866, -74.55136393, -75.61488908,\n",
       "       -77.9977912 , -78.96270609, -78.91256607, -78.97761997,\n",
       "       -78.63520875, -77.41391106, -78.65865717, -78.5880479 ,\n",
       "       -79.55029455, -79.57144899, -79.03375292, -79.57304413,\n",
       "       -79.43336349, -79.40619375, -79.43732266, -78.8714775 ,\n",
       "       -74.77400871, -74.04339931, -76.10942101, -75.81001659,\n",
       "       -78.96652254, -79.46455883, -79.43867932, -79.47225655,\n",
       "       -77.73583938, -75.70966425, -77.77474113, -77.6575978 ,\n",
       "       -79.74576861, -79.75772581, -79.45380069, -79.75862744,\n",
       "       -79.08486447, -79.04098097, -79.09125913, -78.17733038,\n",
       "       -75.39897435, -71.56796295, -76.00809842, -74.57517054,\n",
       "       -79.29827076, -79.63642182, -79.61885046, -79.64164832,\n",
       "       -76.52945892, -73.42365556, -76.58908912, -76.40952701,\n",
       "       -79.82134697, -79.82974817, -79.61620801, -79.83038166,\n",
       "       -78.9657051 , -78.91610676, -78.97293252, -77.9399856 ,\n",
       "       -74.85718872, -67.95875084, -75.22132709, -73.42838537,\n",
       "       -76.29298688, -78.07952658, -77.98669229, -78.10713964,\n",
       "       -79.65637238, -79.34895979, -79.66227459, -79.64450168,\n",
       "       -77.16282755, -77.29631633, -73.90336122, -77.306382  ,\n",
       "       -79.79000979, -79.7799451 , -79.79147644, -79.58186371,\n",
       "       -71.27084627, -74.6586096 , -69.33697975, -74.91406283,\n",
       "       -77.3200267 , -78.611589  , -78.54447533, -78.6315516 ,\n",
       "       -79.48193564, -79.0184037 , -79.49083527, -79.46403629,\n",
       "       -78.87821515, -78.9309921 , -77.58953079, -78.93497172,\n",
       "       -79.65142324, -79.63471155, -79.65385847, -79.30581454,\n",
       "       -74.05420625, -75.95446763, -74.713044  , -76.29841255,\n",
       "       -78.43188123, -79.18758755, -79.14831867, -79.19926787,\n",
       "       -79.13634292, -78.36352296, -79.15118076, -79.10650025,\n",
       "       -79.47108809, -79.4959695 , -78.8635434 , -79.49784567,\n",
       "       -79.30816137, -79.27498699, -79.31299552, -78.62209671,\n",
       "       -75.71157294, -75.60126302, -76.36703279, -75.94469091,\n",
       "       -79.14133724, -79.55512283, -79.53362127, -79.56151836,\n",
       "       -78.64266122, -77.42802566, -78.66598173, -78.59575761,\n",
       "       -79.74553518, -79.7575035 , -79.45329581, -79.75840597,\n",
       "       -78.30657207, -78.2253627 , -78.3184058 , -76.62712178,\n",
       "       -75.74653193, -73.77598264, -76.1591407 , -72.92673948,\n",
       "       -79.39737132, -79.68776223, -79.67267265, -79.69225055,\n",
       "       -78.22461464, -76.63585421, -78.2551182 , -78.16326402,\n",
       "       -79.83160756, -79.83952612, -79.63825351, -79.84012321,\n",
       "       -78.2152044 , -78.12961304, -78.22767667, -76.44513241,\n",
       "       -75.78772265, -72.59661928, -76.08165922, -72.48603403,\n",
       "       -77.6172284 , -78.76554556, -78.7058753 , -78.78329414,\n",
       "       -79.7790779 , -79.58148553, -79.78287161, -79.77144799,\n",
       "       -73.11750146, -73.44133028, -65.21040572, -73.46574846,\n",
       "       -79.86497439, -79.85850531, -79.8659171 , -79.73118674,\n",
       "       -70.1922855 , -72.13177867, -57.71993384, -72.27802551,\n",
       "       -78.49621941, -79.22091608, -79.18325854, -79.23211711,\n",
       "       -79.59895007, -79.24013537, -79.60583917, -79.58509442,\n",
       "       -79.39561415, -79.42404658, -78.70136216, -79.42619052,\n",
       "       -79.70834854, -79.69436676, -79.71038598, -79.41919572,\n",
       "       -76.5044517 , -77.33013908, -76.92726133, -77.55111144,\n",
       "       -78.77320224, -79.36440888, -79.33368791, -79.37354666,\n",
       "       -79.51286026, -79.0769971 , -79.52122866, -79.49602934,\n",
       "       -79.5433106 , -79.56479383, -79.0187411 , -79.56641376,\n",
       "       -79.6228394 , -79.6047565 , -79.62547444, -79.2488737 ,\n",
       "       -76.91732982, -77.24215271, -77.339298  , -77.46299627,\n",
       "       -79.49844406, -79.74012212, -79.72756383, -79.74385753,\n",
       "       -79.04524111, -78.19088523, -79.0616444 , -79.01224986,\n",
       "       -79.85334612, -79.86024175, -79.68496918, -79.86076171,\n",
       "       -76.08331748, -75.89548419, -76.11068831, -72.19883448,\n",
       "       -74.74468121, -73.3726301 , -74.96689343, -66.46742541,\n",
       "       -79.5622285 , -79.77316704, -79.76220607, -79.77642733,\n",
       "       -78.94110158, -77.99354539, -78.95929427, -78.90451136,\n",
       "       -79.87479047, -79.88067725, -79.7310474 , -79.88112114,\n",
       "       -76.80974196, -76.6567467 , -76.83203624, -73.64572582,\n",
       "       -75.42927785, -73.71723814, -75.62716262, -68.73016962,\n",
       "       -78.1890595 , -79.06178057, -79.01643114, -79.07526948,\n",
       "       -79.8320639 , -79.6818939 , -79.83494712, -79.82626523,\n",
       "       -66.89879507, -67.51522558, -51.8470943 , -67.56170729,\n",
       "       -79.89734499, -79.89242856, -79.89806146, -79.79566667,\n",
       "       -66.01658978, -67.12028415, -39.06711814, -67.20350783,\n",
       "       -78.92058743, -79.44076011, -79.41373028, -79.44879997,\n",
       "       -79.67544377, -79.38508471, -79.68101855, -79.66423157,\n",
       "       -79.57449417, -79.59451   , -79.08575484, -79.59601928,\n",
       "       -79.75871955, -79.74715345, -79.76040498, -79.51952389,\n",
       "       -77.43799444, -77.9507156 , -77.75913663, -78.11862502,\n",
       "       -79.00412435, -79.48403813, -79.45910029, -79.49145575,\n",
       "       -79.64948264, -79.33588674, -79.65550356, -79.63737313,\n",
       "       -79.61903996, -79.63696   , -79.18147472, -79.63831125,\n",
       "       -79.73293356, -79.7201306 , -79.73479922, -79.4681594 ,\n",
       "       -77.56251543, -77.92418221, -77.88340388, -78.09205279,\n",
       "       -79.63341371, -79.81004326, -79.8008651 , -79.81277326,\n",
       "       -79.25049059, -78.57981805, -79.26336722, -79.22459235,\n",
       "       -79.8934345 , -79.89844392, -79.77111437, -79.89882165,\n",
       "       -71.89682289, -71.50821187, -71.95345084, -63.86016184,\n",
       "       -71.39868895, -70.53184535, -71.52500446, -53.47195262,\n",
       "       -79.6526525 , -79.8200114 , -79.81131498, -79.82259812,\n",
       "       -79.21908561, -78.52030504, -79.23250191, -79.19210197,\n",
       "       -79.89990407, -79.90460923, -79.78501319, -79.90496402,\n",
       "       -75.12327887, -74.88940343, -75.15735891, -70.28662267,\n",
       "       -74.31206673, -73.19845801, -74.45206227, -63.67817906])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "random_policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\n",
    "policy_eval(random_policy,env,discount_factor=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use policy iteration to improve our policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.]]),\n",
       " array([1978.99901398, 1857.33625444, 1897.07811877, 1837.76287228,\n",
       "        1742.79321998, 1857.33625444, 1706.12160705, 1761.4072642 ,\n",
       "        1818.38527255, 1742.79320113, 1897.07811877, 1761.4072642 ,\n",
       "        1706.12162571, 1742.79320113, 1706.12160705, 1837.76287228,\n",
       "        1999.99901398, 1877.10733758, 1917.25063498, 1857.33624458,\n",
       "        1958.20902384, 1837.7628919 , 1877.10733758, 1818.38524355,\n",
       "        1761.40730228, 1877.10733758, 1724.36526912, 1780.20936754,\n",
       "        1799.20141982, 1724.36526912, 1877.10733758, 1742.79319156,\n",
       "        1724.36528778, 1761.40728344, 1724.36526912, 1857.33624458,\n",
       "        1978.99902384, 1897.07812863, 1897.07812863, 1877.10732772,\n",
       "        1897.07814805, 1780.20938678, 1818.38526298, 1761.40727387,\n",
       "        1818.38528202, 1937.62691398, 1780.20938678, 1837.76288214,\n",
       "        1780.20940562, 1706.12161643, 1857.33626421, 1724.36525965,\n",
       "        1742.79322926, 1780.20938678, 1742.7932106 , 1877.10732772,\n",
       "        1917.25066426, 1958.20901398, 1877.10734734, 1897.07811877,\n",
       "        1877.10736657, 1761.40729291, 1799.20141035, 1742.79320113,\n",
       "        1837.7629206 , 1958.20901398, 1799.20141035, 1857.33625444,\n",
       "        1761.40731157, 1688.06040026, 1837.76290156, 1706.12160705,\n",
       "        1761.40731157, 1799.20141035, 1761.40729291, 1897.07811877,\n",
       "        1897.07815762, 1978.99901398, 1857.33627387, 1917.25063498,\n",
       "        1857.33629291, 1742.79321998, 1780.20939625, 1724.36526912,\n",
       "        1857.33629291, 1978.99901398, 1818.38527255, 1877.10733758,\n",
       "        1742.79323845, 1670.17979626, 1818.38527255, 1688.06039098,\n",
       "        1742.79323845, 1780.20939625, 1742.79321998, 1877.10733758,\n",
       "        1877.10737605, 1999.99901398, 1837.76291113, 1897.07812863,\n",
       "        1958.20902384, 1837.7628919 , 1877.10733758, 1818.38524355,\n",
       "        1761.40730228, 1877.10733758, 1724.36526912, 1780.20936754,\n",
       "        1837.76291113, 1761.40728344, 1917.25063498, 1780.20936754,\n",
       "        1724.36528778, 1761.40728344, 1724.36526912, 1857.33624458,\n",
       "        1978.99902384, 1897.07812863, 1937.62691398, 1877.10732772,\n",
       "        1937.6269336 , 1818.38526298, 1857.33626421, 1799.20139112,\n",
       "        1780.20940562, 1897.07812863, 1742.7932106 , 1799.20139112,\n",
       "        1818.38528202, 1742.7932106 , 1897.07812863, 1761.40727387,\n",
       "        1742.79322926, 1780.20938678, 1742.7932106 , 1877.10732772,\n",
       "        1958.2090336 , 1917.25064484, 1917.25064484, 1897.07811877,\n",
       "        1917.25066426, 1799.20141035, 1837.76290156, 1780.20937721,\n",
       "        1799.2014292 , 1917.25064484, 1761.40729291, 1818.38525332,\n",
       "        1799.2014292 , 1724.3652785 , 1877.10734734, 1742.79320113,\n",
       "        1761.40731157, 1799.20141035, 1761.40729291, 1897.07811877,\n",
       "        1937.62694326, 1937.62692384, 1897.07813839, 1917.25063498,\n",
       "        1897.07815762, 1780.20939625, 1818.38527255, 1761.40728344,\n",
       "        1818.3852914 , 1937.62692384, 1780.20939625, 1837.7628919 ,\n",
       "        1780.20941491, 1706.12162571, 1857.33627387, 1724.36526912,\n",
       "        1780.20941491, 1818.38527255, 1780.20939625, 1917.25063498,\n",
       "        1917.25067383, 1958.20902384, 1877.10735701, 1937.62691398,\n",
       "        1877.10737605, 1761.40730228, 1799.20141982, 1742.7932106 ,\n",
       "        1837.76292998, 1958.20902384, 1799.20141982, 1857.33626421,\n",
       "        1761.40732076, 1688.06040945, 1837.76291113, 1706.12161643,\n",
       "        1761.40732076, 1799.20141982, 1761.40730228, 1897.07812863,\n",
       "        1897.07816709, 1978.99902384, 1857.33628344, 1917.25064484,\n",
       "        1937.6269336 , 1818.38526298, 1857.33626421, 1799.20139112,\n",
       "        1742.79322926, 1857.33626421, 1706.12161643, 1761.40727387,\n",
       "        1857.33628344, 1780.20938678, 1937.62691398, 1799.20139112,\n",
       "        1742.79322926, 1780.20938678, 1742.7932106 , 1877.10732772,\n",
       "        1958.2090336 , 1877.10734734, 1958.20901398, 1897.07811877,\n",
       "        1917.25066426, 1799.20141035, 1837.76290156, 1780.20937721,\n",
       "        1761.40731157, 1877.10734734, 1724.3652785 , 1780.20937721,\n",
       "        1837.7629206 , 1761.40729291, 1917.25064484, 1780.20937721,\n",
       "        1761.40731157, 1799.20141035, 1761.40729291, 1897.07811877,\n",
       "        1937.62694326, 1897.07813839, 1937.62692384, 1917.25063498,\n",
       "        1897.07815762, 1780.20939625, 1818.38527255, 1761.40728344,\n",
       "        1780.20941491, 1897.07813839, 1742.79321998, 1799.20140078,\n",
       "        1818.3852914 , 1742.79321998, 1897.07813839, 1761.40728344,\n",
       "        1780.20941491, 1818.38527255, 1780.20939625, 1917.25063498,\n",
       "        1917.25067383, 1917.2506546 , 1917.2506546 , 1937.62691398,\n",
       "        1877.10737605, 1761.40730228, 1799.20141982, 1742.7932106 ,\n",
       "        1799.20143848, 1917.2506546 , 1761.40730228, 1818.38526298,\n",
       "        1799.20143848, 1724.36528778, 1877.10735701, 1742.7932106 ,\n",
       "        1799.20143848, 1837.76291113, 1799.20141982, 1937.62691398,\n",
       "        1897.07816709, 1937.6269336 , 1897.07814805, 1958.20901398,\n",
       "        1857.33630229, 1742.79322926, 1780.20940562, 1724.3652785 ,\n",
       "        1818.38530068, 1937.6269336 , 1780.20940562, 1837.76290156,\n",
       "        1780.2094241 , 1706.1216349 , 1857.33628344, 1724.3652785 ,\n",
       "        1780.2094241 , 1818.38528202, 1780.20940562, 1917.25064484,\n",
       "        1877.10738542, 1958.2090336 , 1877.10736657, 1937.62692384,\n",
       "        1917.25066426, 1799.20141035, 1837.76290156, 1780.20937721,\n",
       "        1724.36529697, 1837.76290156, 1688.06040026, 1742.79320113,\n",
       "        1877.10736657, 1799.20141035, 1958.20901398, 1818.38525332,\n",
       "        1724.36529697, 1761.40729291, 1724.3652785 , 1857.33625444,\n",
       "        1937.62694326, 1857.33627387, 1978.99901398, 1877.10733758,\n",
       "        1897.07815762, 1780.20939625, 1818.38527255, 1761.40728344,\n",
       "        1742.79323845, 1857.33627387, 1706.12162571, 1761.40728344,\n",
       "        1818.3852914 , 1742.79321998, 1897.07813839, 1761.40728344,\n",
       "        1742.79323845, 1780.20939625, 1742.79321998, 1877.10733758,\n",
       "        1917.25067383, 1877.10735701, 1917.2506546 , 1897.07812863,\n",
       "        1877.10737605, 1761.40730228, 1799.20141982, 1742.7932106 ,\n",
       "        1761.40732076, 1877.10735701, 1724.36528778, 1780.20938678,\n",
       "        1799.20143848, 1724.36528778, 1877.10735701, 1742.7932106 ,\n",
       "        1761.40732076, 1799.20141982, 1761.40730228, 1897.07812863,\n",
       "        1897.07816709, 1897.07814805, 1897.07814805, 1917.25064484,\n",
       "        1857.33630229, 1742.79322926, 1780.20940562, 1724.3652785 ,\n",
       "        1780.2094241 , 1897.07814805, 1742.79322926, 1799.20141035,\n",
       "        1780.2094241 , 1706.1216349 , 1857.33628344, 1724.3652785 ,\n",
       "        1818.38530068, 1857.33628344, 1818.38528202, 1958.20901398,\n",
       "        1877.10738542, 1917.25066426, 1877.10736657, 1978.99901398,\n",
       "        1837.76293926, 1724.36529697, 1761.40731157, 1706.12162571,\n",
       "        1799.20144767, 1917.25066426, 1761.40731157, 1818.38527255,\n",
       "        1761.40732986, 1688.06041855, 1837.7629206 , 1706.12162571,\n",
       "        1799.20144767, 1837.7629206 , 1799.2014292 , 1937.62692384,\n",
       "        1857.33631157, 1937.62694326, 1857.33629291, 1958.20902384,\n",
       "        1897.07815762, 1780.20939625, 1818.38527255, 1761.40728344,\n",
       "        1706.121644  , 1818.38527255, 1670.17979626, 1724.36526912,\n",
       "        1897.07815762, 1818.38527255, 1978.99901398, 1837.7628919 ,\n",
       "        1706.121644  , 1742.79321998, 1706.12162571, 1837.7628919 ,\n",
       "        1917.25067383, 1837.76291113, 1999.99901398, 1857.33626421,\n",
       "        1877.10737605, 1761.40730228, 1799.20141982, 1742.7932106 ,\n",
       "        1724.36530607, 1837.76291113, 1688.06040945, 1742.7932106 ,\n",
       "        1799.20143848, 1724.36528778, 1877.10735701, 1742.7932106 ,\n",
       "        1724.36530607, 1761.40730228, 1724.36528778, 1857.33626421,\n",
       "        1897.07816709, 1857.33628344, 1897.07814805, 1877.10734734,\n",
       "        1857.33630229, 1742.79322926, 1780.20940562, 1724.3652785 ,\n",
       "        1742.79324755, 1857.33628344, 1706.1216349 , 1761.40729291,\n",
       "        1780.2094241 , 1706.1216349 , 1857.33628344, 1724.3652785 ,\n",
       "        1742.79324755, 1780.20940562, 1742.79322926, 1877.10734734,\n",
       "        1877.10738542, 1877.10736657, 1877.10736657, 1897.07813839,\n",
       "        1837.76293926, 1724.36529697, 1761.40731157, 1706.12162571,\n",
       "        1761.40732986, 1877.10736657, 1724.36529697, 1780.20939625,\n",
       "        1761.40732986, 1688.06041855, 1837.7629206 , 1706.12162571,\n",
       "        1837.76293926, 1877.10736657, 1837.7629206 , 1978.99901398,\n",
       "        1857.33631157, 1897.07815762, 1857.33629291, 1999.99901398,\n",
       "        1818.38530987, 1706.121644  , 1742.79323845, 1688.06040945,\n",
       "        1780.2094332 , 1897.07815762, 1742.79323845, 1799.20141982,\n",
       "        1742.79325656, 1670.17981437, 1818.3852914 , 1688.06040945,\n",
       "        1818.38530987, 1857.33629291, 1818.3852914 , 1958.20902384,\n",
       "        1837.76294845, 1917.25067383, 1837.76292998, 1978.99902384]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_iter_policy = policy_iteration(env,policy_eval,discount_factor=0.99)\n",
    "pol_iter_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use value iteration to improve our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.]]),\n",
       " array([1978.99015022, 1857.32747932, 1897.06925501, 1837.75400852,\n",
       "        1742.78487492, 1857.32747932, 1706.1131777 , 1761.39857683,\n",
       "        1818.37675805, 1742.78468664, 1897.06925501, 1761.39857683,\n",
       "        1706.11336411, 1742.78468664, 1706.1131777 , 1837.75400852,\n",
       "        1999.99015022, 1877.09856246, 1917.24177122, 1857.32738082,\n",
       "        1958.20024872, 1837.75420453, 1877.09856246, 1818.37646843,\n",
       "        1761.39895723, 1877.09856246, 1724.35683977, 1780.20068017,\n",
       "        1799.19299047, 1724.35683977, 1877.09856246, 1742.78459106,\n",
       "        1724.35702617, 1761.39876894, 1724.35683977, 1857.32738082,\n",
       "        1978.99024872, 1897.06935351, 1897.06935351, 1877.09846396,\n",
       "        1897.06954756, 1780.20087228, 1818.37666248, 1761.39867337,\n",
       "        1818.37685267, 1937.61805022, 1780.20087228, 1837.75410701,\n",
       "        1780.20106057, 1706.11327137, 1857.32757683, 1724.35674515,\n",
       "        1742.78496766, 1780.20087228, 1742.78478125, 1877.09846396,\n",
       "        1917.24206377, 1958.20015022, 1877.09865997, 1897.06925501,\n",
       "        1877.09885208, 1761.39886356, 1799.19289586, 1742.78468664,\n",
       "        1837.75449125, 1958.20015022, 1799.19289586, 1857.32747932,\n",
       "        1761.39904996, 1688.05213866, 1837.75430106, 1706.1131777 ,\n",
       "        1761.39904996, 1799.19289586, 1761.39886356, 1897.06925501,\n",
       "        1897.06964313, 1978.99015022, 1857.32767337, 1917.24177122,\n",
       "        1857.32786356, 1742.78487492, 1780.2009669 , 1724.35683977,\n",
       "        1857.32786356, 1978.99015022, 1818.37675805, 1877.09856246,\n",
       "        1742.78505946, 1670.17161727, 1818.37675805, 1688.05204592,\n",
       "        1742.78505946, 1780.2009669 , 1742.78487492, 1877.09856246,\n",
       "        1877.0989467 , 1999.99015022, 1837.75439664, 1897.06935351,\n",
       "        1958.20024872, 1837.75420453, 1877.09856246, 1818.37646843,\n",
       "        1761.39895723, 1877.09856246, 1724.35683977, 1780.20068017,\n",
       "        1837.75439664, 1761.39876894, 1917.24177122, 1780.20068017,\n",
       "        1724.35702617, 1761.39876894, 1724.35683977, 1857.32738082,\n",
       "        1978.99024872, 1897.06935351, 1937.61805022, 1877.09846396,\n",
       "        1937.61824623, 1818.37666248, 1857.32757683, 1799.19270375,\n",
       "        1780.20106057, 1897.06935351, 1742.78478125, 1799.19270375,\n",
       "        1818.37685267, 1742.78478125, 1897.06935351, 1761.39867337,\n",
       "        1742.78496766, 1780.20087228, 1742.78478125, 1877.09846396,\n",
       "        1958.20034623, 1917.24186972, 1917.24186972, 1897.06925501,\n",
       "        1917.24206377, 1799.19289586, 1837.75430106, 1780.20077671,\n",
       "        1799.19308414, 1917.24186972, 1761.39886356, 1818.37656594,\n",
       "        1799.19308414, 1724.35693344, 1877.09865997, 1742.78468664,\n",
       "        1761.39904996, 1799.19289586, 1761.39886356, 1897.06925501,\n",
       "        1937.61834277, 1937.61814872, 1897.06945102, 1917.24177122,\n",
       "        1897.06964313, 1780.2009669 , 1818.37675805, 1761.39876894,\n",
       "        1818.37694634, 1937.61814872, 1780.2009669 , 1837.75420453,\n",
       "        1780.2011533 , 1706.11336411, 1857.32767337, 1724.35683977,\n",
       "        1780.2011533 , 1818.37675805, 1780.2009669 , 1917.24177122,\n",
       "        1917.24215934, 1958.20024872, 1877.09875651, 1937.61805022,\n",
       "        1877.0989467 , 1761.39895723, 1799.19299047, 1742.78478125,\n",
       "        1837.75458492, 1958.20024872, 1799.19299047, 1857.32757683,\n",
       "        1761.39914177, 1688.05223046, 1837.75439664, 1706.11327137,\n",
       "        1761.39914177, 1799.19299047, 1761.39895723, 1897.06935351,\n",
       "        1897.06973775, 1978.99024872, 1857.32776894, 1917.24186972,\n",
       "        1937.61824623, 1818.37666248, 1857.32757683, 1799.19270375,\n",
       "        1742.78496766, 1857.32757683, 1706.11327137, 1761.39867337,\n",
       "        1857.32776894, 1780.20087228, 1937.61805022, 1799.19270375,\n",
       "        1742.78496766, 1780.20087228, 1742.78478125, 1877.09846396,\n",
       "        1958.20034623, 1877.09865997, 1958.20015022, 1897.06925501,\n",
       "        1917.24206377, 1799.19289586, 1837.75430106, 1780.20077671,\n",
       "        1761.39904996, 1877.09865997, 1724.35693344, 1780.20077671,\n",
       "        1837.75449125, 1761.39886356, 1917.24186972, 1780.20077671,\n",
       "        1761.39904996, 1799.19289586, 1761.39886356, 1897.06925501,\n",
       "        1937.61834277, 1897.06945102, 1937.61814872, 1917.24177122,\n",
       "        1897.06964313, 1780.2009669 , 1818.37675805, 1761.39876894,\n",
       "        1780.2011533 , 1897.06945102, 1742.78487492, 1799.19280028,\n",
       "        1818.37694634, 1742.78487492, 1897.06945102, 1761.39876894,\n",
       "        1780.2011533 , 1818.37675805, 1780.2009669 , 1917.24177122,\n",
       "        1917.24215934, 1917.24196723, 1917.24196723, 1937.61805022,\n",
       "        1877.0989467 , 1761.39895723, 1799.19299047, 1742.78478125,\n",
       "        1799.19317688, 1917.24196723, 1761.39895723, 1818.37666248,\n",
       "        1799.19317688, 1724.35702617, 1877.09875651, 1742.78478125,\n",
       "        1799.19317688, 1837.75439664, 1799.19299047, 1937.61805022,\n",
       "        1897.06973775, 1937.61824623, 1897.06954756, 1958.20015022,\n",
       "        1857.32795723, 1742.78496766, 1780.20106057, 1724.35693344,\n",
       "        1818.37703907, 1937.61824623, 1780.20106057, 1837.75430106,\n",
       "        1780.20124511, 1706.11345591, 1857.32776894, 1724.35693344,\n",
       "        1780.20124511, 1818.37685267, 1780.20106057, 1917.24186972,\n",
       "        1877.09904037, 1958.20034623, 1877.09885208, 1937.61814872,\n",
       "        1917.24206377, 1799.19289586, 1837.75430106, 1780.20077671,\n",
       "        1724.35711798, 1837.75430106, 1688.05213866, 1742.78468664,\n",
       "        1877.09885208, 1799.19289586, 1958.20015022, 1818.37656594,\n",
       "        1724.35711798, 1761.39886356, 1724.35693344, 1857.32747932,\n",
       "        1937.61834277, 1857.32767337, 1978.99015022, 1877.09856246,\n",
       "        1897.06964313, 1780.2009669 , 1818.37675805, 1761.39876894,\n",
       "        1742.78505946, 1857.32767337, 1706.11336411, 1761.39876894,\n",
       "        1818.37694634, 1742.78487492, 1897.06945102, 1761.39876894,\n",
       "        1742.78505946, 1780.2009669 , 1742.78487492, 1877.09856246,\n",
       "        1917.24215934, 1877.09875651, 1917.24196723, 1897.06935351,\n",
       "        1877.0989467 , 1761.39895723, 1799.19299047, 1742.78478125,\n",
       "        1761.39914177, 1877.09875651, 1724.35702617, 1780.20087228,\n",
       "        1799.19317688, 1724.35702617, 1877.09875651, 1742.78478125,\n",
       "        1761.39914177, 1799.19299047, 1761.39895723, 1897.06935351,\n",
       "        1897.06973775, 1897.06954756, 1897.06954756, 1917.24186972,\n",
       "        1857.32795723, 1742.78496766, 1780.20106057, 1724.35693344,\n",
       "        1780.20124511, 1897.06954756, 1742.78496766, 1799.19289586,\n",
       "        1780.20124511, 1706.11345591, 1857.32776894, 1724.35693344,\n",
       "        1818.37703907, 1857.32776894, 1818.37685267, 1958.20015022,\n",
       "        1877.09904037, 1917.24206377, 1877.09885208, 1978.99015022,\n",
       "        1837.75467766, 1724.35711798, 1761.39904996, 1706.11336411,\n",
       "        1799.19326868, 1917.24206377, 1761.39904996, 1818.37675805,\n",
       "        1761.39923266, 1688.05232135, 1837.75449125, 1706.11336411,\n",
       "        1799.19326868, 1837.75449125, 1799.19308414, 1937.61814872,\n",
       "        1857.32804996, 1937.61834277, 1857.32786356, 1958.20024872,\n",
       "        1897.06964313, 1780.2009669 , 1818.37675805, 1761.39876894,\n",
       "        1706.1135468 , 1818.37675805, 1670.17161727, 1724.35683977,\n",
       "        1897.06964313, 1818.37675805, 1978.99015022, 1837.75420453,\n",
       "        1706.1135468 , 1742.78487492, 1706.11336411, 1837.75420453,\n",
       "        1917.24215934, 1837.75439664, 1999.99015022, 1857.32757683,\n",
       "        1877.0989467 , 1761.39895723, 1799.19299047, 1742.78478125,\n",
       "        1724.35720887, 1837.75439664, 1688.05223046, 1742.78478125,\n",
       "        1799.19317688, 1724.35702617, 1877.09875651, 1742.78478125,\n",
       "        1724.35720887, 1761.39895723, 1724.35702617, 1857.32757683,\n",
       "        1897.06973775, 1857.32776894, 1897.06954756, 1877.09865997,\n",
       "        1857.32795723, 1742.78496766, 1780.20106057, 1724.35693344,\n",
       "        1742.78515035, 1857.32776894, 1706.11345591, 1761.39886356,\n",
       "        1780.20124511, 1706.11345591, 1857.32776894, 1724.35693344,\n",
       "        1742.78515035, 1780.20106057, 1742.78496766, 1877.09865997,\n",
       "        1877.09904037, 1877.09885208, 1877.09885208, 1897.06945102,\n",
       "        1837.75467766, 1724.35711798, 1761.39904996, 1706.11336411,\n",
       "        1761.39923266, 1877.09885208, 1724.35711798, 1780.2009669 ,\n",
       "        1761.39923266, 1688.05232135, 1837.75449125, 1706.11336411,\n",
       "        1837.75467766, 1877.09885208, 1837.75449125, 1978.99015022,\n",
       "        1857.32804996, 1897.06964313, 1857.32786356, 1999.99015022,\n",
       "        1818.37713088, 1706.1135468 , 1742.78505946, 1688.05223046,\n",
       "        1780.201336  , 1897.06964313, 1742.78505946, 1799.19299047,\n",
       "        1742.78524033, 1670.17179814, 1818.37694634, 1688.05223046,\n",
       "        1818.37713088, 1857.32786356, 1818.37694634, 1958.20024872,\n",
       "        1837.75476946, 1917.24215934, 1837.75458492, 1978.99024872]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_iter_policy = value_iteration(env,discount_factor=0.99)\n",
    "val_iter_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Policy\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(pol_iter_policy[0])):\n",
    "    if not (pol_iter_policy[0][x] == val_iter_policy[0][x]).all():\n",
    "        print(\"Not the same Policy\")\n",
    "        break\n",
    "print(\"Same Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that policy-iterated and value-iterated policies would converge to the same policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(policy):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    reward = None\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(np.argmax(policy[curr_state]))  \n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An agent using a policy which has been improved using policy-iterated takes about an average of 14 steps to successfully complete its mission.\n"
     ]
    }
   ],
   "source": [
    "optimal_counts = count(pol_iter_policy[0])\n",
    "print(\"An agent using a policy which has been improved using policy-iterated takes about an average of \" + str(int(np.mean(optimal_counts)))\n",
    "      + \" steps to successfully complete its mission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An agent using a policy which has been value-iterated takes about an average of 13 steps to successfully complete its mission.\n"
     ]
    }
   ],
   "source": [
    "optimal_counts1 = count(val_iter_policy[0])\n",
    "print(\"An agent using a policy which has been value-iterated takes about an average of \" + str(int(np.mean(optimal_counts1)))\n",
    "      + \" steps to successfully complete its mission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the improved policy is definitely much more efficient than the random search.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Let's watch how both our optimal policy works in action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_policy(policy):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    reward = None\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(np.argmax(policy[0][curr_state])) \n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "        env.env.s = curr_state\n",
    "        env.render()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[42mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[42m_\u001b[0m|\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m:\u001b[42m_\u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "view_policy(pol_iter_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[42mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "view_policy(val_iter_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
